{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc9fc2c",
   "metadata": {},
   "source": [
    "Création et enregistrement des dictionnaires d'occurences par classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "types = defaultdict(Counter)\n",
    "belonging = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "def sentences_to_list_of_words(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "with open('company-document-text.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    type_nb = {\n",
    "        \"invoice\" : 0,\n",
    "        \"purchase Order\": 0,\n",
    "        \"report\":0,\n",
    "        \"ShippingOrder\":0\n",
    "    }\n",
    "    for idx, line in enumerate(reader):\n",
    "        label = line[\"label\"]\n",
    "        type_nb[label] += 1\n",
    "        words = sentences_to_list_of_words(line[\"text\"])\n",
    "        unique_words = set(words)  # Pour éviter de compter 2 fois le même mot dans une ligne\n",
    "\n",
    "        types[label].update(words)  # Occurrence totale\n",
    "\n",
    "        for word in unique_words:\n",
    "            belonging[word][label] += 1  # Appartenance (nb de lignes)\n",
    "\n",
    "# Écriture triée\n",
    "for label in types:\n",
    "    filename = f\"./occurences/{label}_output.csv\"\n",
    "    word_data = []\n",
    "\n",
    "    for word, freq in types[label].items():\n",
    "        b = belonging[word][label]\n",
    "        score = freq * b\n",
    "        word_data.append((word, freq, b, score))\n",
    "\n",
    "    # Tri rapide avec Python natif\n",
    "    word_data.sort(key=lambda x: (-x[2], -x[1]))  # Tri par appartenance décroissante puis fréquence\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Word', 'Frequency', 'Belonging', 'Score'])\n",
    "        writer.writerows(word_data)\n",
    "\n",
    "print(type_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c876587",
   "metadata": {},
   "source": [
    "Filtre à + ou - 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b235f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file = \"./occurences\"\n",
    "percent_1 = 0.20\n",
    "\n",
    "total_invoice = type_nb['invoice']\n",
    "total_po = type_nb['purchase Order']\n",
    "total_report = type_nb['report']\n",
    "total_shipping = type_nb['ShippingOrder']\n",
    "\n",
    "def load_word_counts(filename):\n",
    "    counts = {}\n",
    "    with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            word = row['Word']\n",
    "            count = int(row['Belonging'])\n",
    "            counts[word] = count\n",
    "    return counts\n",
    "\n",
    "def is_within_margin(x, y):\n",
    "    margin = x * percent_1\n",
    "    return (x - margin) <= y <= (x + margin)\n",
    "\n",
    "# Chargement des dictionnaires de chaque type\n",
    "type1 = load_word_counts(f'{file}/invoice_output.csv')\n",
    "type2 = load_word_counts(f'{file}/purchase Order_output.csv')\n",
    "type3 = load_word_counts(f'{file}/report_output.csv')\n",
    "type4 = load_word_counts(f'{file}/ShippingOrder_output.csv')\n",
    "\n",
    "# Résultat : mots à supprimer\n",
    "mots_trop_communs = []\n",
    "mots_trop_rares = set()\n",
    "\n",
    "percent_2 = 0.10\n",
    "\n",
    "# Détection des mots trop rares (moins de 10% du nombre de fichiers par catégorie)\n",
    "for word, count in type1.items():\n",
    "    if count < total_invoice * percent_2:\n",
    "        mots_trop_rares.add(word)\n",
    "\n",
    "for word, count in type2.items():\n",
    "    if count < total_po * percent_2:\n",
    "        mots_trop_rares.add(word)\n",
    "\n",
    "for word, count in type3.items():\n",
    "    if count < total_report * percent_2:\n",
    "        mots_trop_rares.add(word)\n",
    "\n",
    "for word, count in type4.items():\n",
    "    if count < total_shipping * percent_2:\n",
    "        mots_trop_rares.add(word)\n",
    "\n",
    "# Supprimer les mots (de la liste des mots à supprimer) qui apparaissent dans plus de 10% des fichiers d'une catégorie\n",
    "for word in list(mots_trop_rares):\n",
    "    if (word in type1 and type1[word] > total_invoice * percent_2) or \\\n",
    "       (word in type2 and type2[word] > total_po * percent_2) or \\\n",
    "       (word in type3 and type3[word] > total_report * percent_2) or \\\n",
    "       (word in type4 and type4[word] > total_shipping * percent_2):\n",
    "        mots_trop_rares.remove(word)\n",
    "\n",
    "# Résultat : mots trop communs\n",
    "for word in type1:\n",
    "    if word in type2 and word in type3 and word in type4:\n",
    "        c1 = type1[word]\n",
    "        c2 = type2[word]\n",
    "        c3 = type3[word]\n",
    "        c4 = type4[word]\n",
    "\n",
    "        if all(is_within_margin(c1, c) for c in [c2, c3, c4]):\n",
    "            mots_trop_communs.append(word)\n",
    "\n",
    "mots_trop_rares = list(mots_trop_rares)\n",
    "\n",
    "print(f\"{len(mots_trop_communs)} mots trop communs trouvés.\")\n",
    "print(f\"{len(mots_trop_rares)} mots trop rares trouvés.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7207b0",
   "metadata": {},
   "source": [
    "nettoyage des fichiers csv on enlève les mots trop commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd95acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "file2 = \"./occurences/nettoyer\"\n",
    "\n",
    "mots_a_supprimer = mots_trop_rares + mots_trop_communs\n",
    "\n",
    "def nettoyer_fichier(input_filename, output_filename, mots_a_supprimer):\n",
    "    with open(input_filename, newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_filename, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            if row['Word'] not in mots_a_supprimer:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Fichiers à nettoyer\n",
    "fichiers = [\n",
    "    f'{file}/invoice_output.csv',\n",
    "    f'{file}/purchase Order_output.csv',\n",
    "    f'{file}/report_output.csv',\n",
    "    f'{file}/ShippingOrder_output.csv'\n",
    "]\n",
    "\n",
    "for f in fichiers:\n",
    "    nom_fichier = os.path.basename(f).replace('.csv', '_nettoye.csv')\n",
    "    chemin_sortie = os.path.join(file2, nom_fichier)\n",
    "    nettoyer_fichier(f, chemin_sortie, mots_a_supprimer)\n",
    "\n",
    "print(\"Nettoyage terminé ! Fichiers sauvegardés dans le dossier 'nettoyer'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e06a27",
   "metadata": {},
   "source": [
    "Création du Dataset d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file = \"./occurences\"\n",
    "file2 = \"./occurences/nettoyer\"\n",
    "fichier_principal = \"company-document-text.csv\"\n",
    "fichier_mots = [\n",
    "    f'{file2}/invoice_output_nettoye.csv',\n",
    "    f'{file2}/purchase Order_output_nettoye.csv',\n",
    "    f'{file2}/report_output_nettoye.csv',\n",
    "    f'{file2}/ShippingOrder_output_nettoye.csv'\n",
    "]\n",
    "fichier_sortie = \"training_data_set.csv\"\n",
    "colonne_texte = \"Word\"\n",
    "pourcentage = 0.8\n",
    "max_words = 800\n",
    "\n",
    "def load_words_from_file(file_path):\n",
    "    words = []\n",
    "    with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            word = row['Word']\n",
    "            words.append(word)\n",
    "            count += 1\n",
    "            if count >= max_words:\n",
    "                break\n",
    "    return words\n",
    "\n",
    "# Charger tous les mots des fichiers spécifiés\n",
    "mots = []\n",
    "for fichier in fichier_mots:\n",
    "    mots += load_words_from_file(fichier)\n",
    "\n",
    "# Créer la liste des features, y compris les mots extraits\n",
    "features = list(set(mots)) + ['word_count', 'invoice', 'purchase Order', 'report', 'ShippingOrder']\n",
    "\n",
    "print(len(features))\n",
    "\n",
    "# Variables pour accumuler les sommes des colonnes\n",
    "somme_colonnes = {feature: 0 for feature in features[1:-1]}  # Exclure 'text' et 'word_count'\n",
    "\n",
    "# Lire et modifier le fichier CSV principal\n",
    "with open(fichier_principal, mode='r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    # Créer un fichier de sortie avec les modifications\n",
    "    with open(fichier_sortie, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=features)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in reader:\n",
    "            texte = row[reader.fieldnames[0]]  # Récupérer le texte dans la première colonne\n",
    "            output_row = {feature: 0 for feature in features}\n",
    "\n",
    "            # Extraire les mots avec re\n",
    "            mots_dans_texte = re.findall(r'\\b\\w+\\b', texte.lower())\n",
    "\n",
    "            # Compter les mots pertinents\n",
    "            for mot in mots_dans_texte:\n",
    "                if mot in mots:\n",
    "                    output_row[mot] += 1\n",
    "\n",
    "            # Ajouter le label\n",
    "            if row['label'] in output_row:\n",
    "                output_row[row['label']] = 1\n",
    "\n",
    "            # Word count (à convertir si nécessaire)\n",
    "            output_row['word_count'] = int(row['word_count'])\n",
    "\n",
    "            # Mettre à jour les sommes\n",
    "            for feature in somme_colonnes:\n",
    "                somme_colonnes[feature] += int(output_row[feature])\n",
    "\n",
    "            # Écrire la ligne modifiée\n",
    "            writer.writerow(output_row)\n",
    "\n",
    "# Afficher la somme de chaque colonne\n",
    "print(\"Somme des colonnes :\")\n",
    "for feature, total in somme_colonnes.items():\n",
    "    print(f\"{feature}: {total}\")\n",
    "\n",
    "print(f\"Le fichier {fichier_sortie} a été créé avec succès.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4f7cb",
   "metadata": {},
   "source": [
    "Création du modèle XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67566a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Charger ton jeu de données\n",
    "data = pd.read_csv(\"training_data_set.csv\")\n",
    "\n",
    "# Supposons que chaque étiquette (label) est sous forme de quadruplet (1 ou 0 pour chaque type)\n",
    "# Exemple : 'invoice', 'purchase Order', 'report', 'ShippingOrder'\n",
    "X = data.drop(columns=['invoice', 'purchase Order', 'report', 'ShippingOrder'])  # Les caractéristiques\n",
    "y = data[['invoice', 'purchase Order', 'report', 'ShippingOrder']]  # Les 4 colonnes de labels\n",
    "\n",
    "# Diviser en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser le modèle XGBoost et l'encoder dans un classificateur multi-output\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# MultiOutputClassifier permet de traiter plusieurs sorties simultanément\n",
    "model = MultiOutputClassifier(xgb_model, n_jobs=-1)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur le jeu de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Évaluer les performances du modèle\n",
    "print(\"Précision globale : \", accuracy_score(y_test, y_pred))\n",
    "print(\"Rapport de classification :\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Enregistre ton modèle entraîné dans un fichier\n",
    "joblib.dump(model, \"xgboost_modele.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abadaa3e",
   "metadata": {},
   "source": [
    "Réutilisation du modèle pour un fichier donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55582003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = joblib.load(\"xgboost_modele.pkl\")\n",
    "\n",
    "vocab = list(set(mots))\n",
    "\n",
    "def vectorize(doc, vocab):\n",
    "    word_total = 0\n",
    "    vec = [0] * len(vocab)\n",
    "    for word in doc:\n",
    "        word_total += 1\n",
    "        if word in vocab:\n",
    "            vec[vocab.index(word)] += 1\n",
    "    vec.append(word_total)\n",
    "    return vec\n",
    "\n",
    "# Exemple de document brut\n",
    "texte = 'order id  10326 shipping details  ship name  bólido comidas preparadas ship address  c  araquil, 67 ship city  madrid ship region  southern europe ship postal code  28023 ship country  spain customer details  customer id  bolid customer name  bólido comidas preparadas employee details  employee name  margaret peacock shipper details  shipper id  2 shipper name  united package order details  order date  2016-10-10 shipped date  2016-10-14 products  -------------------------------------------------------------------------------------------------- product  chef anton s cajun seasoning quantity  24 unit price  17 6 total  422 40000000000003 -------------------------------------------------------------------------------------------------- product  ravioli angelo quantity  16 unit price  15 6 total  249 6 -------------------------------------------------------------------------------------------------- product  rhönbräu klosterbier quantity  50 unit price  6 2 total  310 0 total price  total price  982 0'\n",
    "mots_dans_texte = re.findall(r'\\b\\w+\\b', texte.lower())\n",
    "\n",
    "# Vectorisation\n",
    "x = vectorize(mots_dans_texte, vocab)\n",
    "print(x)\n",
    "# === 4. Prédiction ===\n",
    "\n",
    "y_pred = model.predict([x])[0]  # [x] car il attend une liste de vecteurs\n",
    "\n",
    "# === 5. Interprétation du résultat ===\n",
    "\n",
    "labels = [\"invoice\", \"purchase Order\", \"report\", \"ShippingOrder\"]\n",
    "predicted_labels = [labels[i] for i, v in enumerate(y_pred) if v == 1]\n",
    "\n",
    "# === 6. Affichage du résultat ===\n",
    "\n",
    "if predicted_labels:\n",
    "    print(\"✅ Type de document détecté :\", predicted_labels[0])\n",
    "else:\n",
    "    print(\"❌ Aucun type détecté\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
